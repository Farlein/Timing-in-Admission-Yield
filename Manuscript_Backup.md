

quarto
---
title: "Potential Materials for Manuscript"
output: word_document
bibliography: Bib_20220818.bib
pandoc_args: ["--filter=pandoc-citeproc"]
---

## Introduction
Motivation
Estimating the tuition revenue of undergraduate students is a critical part of the budgetary planning [@trusheim2011predictive], as higher education institutions become more reliant on tuition to support operation [@barringer2016changing; @mitchell2016funding]. With tuition rates usualy being stable or stably increasing, predicting undergraduate student tuition revenue relies on acurate enrollment projection. Enrollment projection can be divided into two parts, admission of new students and retention of continuing students. In this project, we build a tool to predict weekly yield in the admission season, between end of Feburary and beginning of May. Institutional administrative units such as the Budget Office and the Admission Office can use this tool to monitor the gap between admission targets and potential outcomes and adjust the expected tuition revenue from incoming students accordingly.

Summary of previous studies
Although predicting admission yield is an important task of enrollment management in higher education institutions, there is scant literature available to describe the modeling strategy. Administrative units of the institutions often consider themselves lack of analytic capabilities and have to relie on consulting companies for the prediction task [@goenner2006predictive], but consulting companies typically view their models as proprietary and reluctant to disclose the modeling details [@desjardins2002analytic]. That being said, there were still intermissive efforts to study admission yield prediction in recent two decades [@aulck2019using; @chang2006applying; @desjardins2002analytic; @goenner2006predictive; @jamison2017applying; @sarafraz2015student;@shrestha2016offer]. Various types of predictive models are used in previous studies, including logistic regression [@chang2006applying; @desjardins2002analytic; @goenner2006predictive; @shrestha2016offer], neural network [@aulck2019using; @chang2006applying; sarafraz2015student; @shrestha2016offer] and tree-based models [@chang2006applying; @jamison2017applying]. The models were built based on training dataset and their performance was evaluated in on test dataset (out-of-sample test). The primary objective is to determine or improve the predictive accuracy of the models in the out-of-sample test, instead of investigating the effects of the input variables. 

Difference from previous studies
We adopt a similar framework from previous studies that we build predictive models for admission yield and test the accuracy out of sample, but we improve the framework in several important aspects. 
First, we predict weekly yields between March and May instead of the final yield. The final yield is surely with the most concern, but it is a better tool if the Admission Office are able to review the predicted yield by week. They gain more confidence of the tool when they can validate the prediction accuracy early in the admission season rather than waiting until the final yield is settled. They can understand better the flow of deposit with the weekly yields and make adjustment to admission decisions swiftly. 
Second, our prediction is active but not static. We update the weekly prediction based on newly available deposit data from the target cohort of applicants every week, instead of making prediction once and done as of a chosen date. The closer to the end of admission season, the more accurate prediction the Admission Office have. 
Third, as a result of the previous consideration, our model is a combination of exponential survival model and logistic regression model. The exponential survival part takes time as input and the logistic regression parts takes applicants' attributes as input, so the output is determined by time such as which week in the admission season and applicants including the academic, demographic and financial information. 
Fourth, we improve prediction accuracy with relatively novel tranfer learning approach [@pan2009survey; @weiss2016survey]. Given the purpose of a model is to predict the yield of a target cohort of applicants, a classical approach build the model based on a historical cohort but does not utilize information from the target cohort, although Feburary deposit data of the target cohort have already been available. A transfer learning approach takes the model built from the historical cohort as input and update the model based on available information from the target cohort, so the update model tends to be more accurate due to the extra information. 
Fifth, we perform variable selection with BootStrap Lasso (BoLasso) [@bach2008bolasso], in order to enhance the models' robustness. This step is to assure input variables have consistent predictive power to suppress overfitting. 
Sixth, we do not assume all input variables are time indepedent. Since our prediction involves target cohort's attributes, we need to predict the the attributes in late weeks based on those in early weeks. It is fine to assume some attributes do not change over time such as applcant's demographic information, but institutional financial aid offered to applicants could change, because the Admission Office can reassign the offered financial aid from applicants who reject the admission offers or just increase the financial aid for highly desired applicants at the end of admission season. 
Seventh, we use one cohort to build a model and use another cohort to test the model in a out-of-sample test. Previous studies randomly split data from a single cohort or mixed cohorts into at least two parts, one part to build a model and the other to test the model. We argue that our out-of-sample test is more realistic. A new cohort never respond to the admission decisions in exactly the same way with the previous cohorts, and we never know how differently a new cohort will behave due to change of admission policies over time or sudden change of macro environment. Either using a single cohort or mixing cohorts together does not reflect enough the modeling challenge from the difference among cohorts and thus potentially does not evaludate the model's performance properly. 

Summary of results
In this paper, we implement the improved framework to nine fall cohorts of applicants to the University of Delaware, from Fall 2011 to Fall 2020. Our models show high accuracy in terms of the difference between the number of predicted deposits and actual deposits by week. The typical error rate is below 3% or below 150 deposits. The closer a future week is to the current week, the more accurate the prediction is. We show that transfer learning helps to enhance the models' predictive accuracy, by comparing the performance of models before and after being updated from target cohort's information. The increase of accuracy depends on the difference between the cohort used to develop a model and the cohort for testing the model. The impact of transfer learning is trivial, if the difference is neglectable. Otherwise, transfer learning is necessary to guarantee this tool is helpful for institutional planning on tuition revenue from incoming students. The accuracy of predicting fall 2020 is an exception though. Unless April's data are available, the accuracy is low for predicting yield in May no matter how the model configuration is tuned.


### Last Version ###
It is challenging to build a robust model to predict admission yield, because of potential overfitting, change of admission policies over time or sudden change of macro environment. There are intermissive efforts on admission yield prediction, which usually use out-of-sample tests to evaludate the robustness of the predictive models []. An out-of-sample test requires that the dataset used to test the performance of a model is not used at all when building the model. The dataset used to build the model is called training dataset, and the dataset used to test the model is called test dataset. An overfitted model typically shows high performance from the training dataset, but its performance is much inferior when applied to the test dataset, i.e., in an out-of-sample test. In previous studies, training and test datasets were generated by randomly splitting the available dataset into two datasets. Aulck et al. used a 80-20 training-test split to predict the admission yield of 71,699 applicants between 2014 and 2017 in a public university [@aulck2019using]. They trained 7 predictive classifiers with the training dataset, and tested their performance with the test dataset. All 7 classifiers were robust, with gradient boosted trees, random forests and a neural network with 3 hidden layers performing slightly better. Sarafraz et al. predicted the admission yield of 4082 applicants with a two-layer feed-forward neural network [@sarafraz2015student]. They used a 70-15-15 training-validation-test split and chose the best model using an average of the percentage of correct predictions from all three datasets. Goenner and Pauls used a 50-50 training-test split to predict the yield from 15,827 inquiriers for the fall of 2003 in the University of North Dakota [@goenner2006predictive]. They built multiple logistic regression models from the training dataset, utilized Bayesian model averaging to generate the final model specification, and tested the final model with the test dataset. In this project we also use out-of-sample tests to evalute the models' performance. However, we argue that the regular data split method could undermine the purpose of a out-of-sample test. Admission yield prediction is only meaningful for a new cohort of applicants, and an out-of-sample test is to evaluate a model's potential performance for the new cohort, so the test dataset is supposed to be as independent as possible with the traning dataset. The independence decreases between training and test datasets if they are drawing from the same population, i.e., all available cohorts. Therefore, we use one cohort of applicants' data as training dataset and use another cohort as test dataset, instead of randomly drawing samples from all available cohorts.  

An out-of-sample test helps to detect overfitting and thus helps to increase the robustness when building a predictive model on admssion yield, but it cannot helps to enhance the robustness when a new cohort of admitted students behave distinctly differently from the cohort(s) a model is built from. Similar applicants who accepted an institution's admission offer last year may have non-neglectable lower chance to accpet an offer this year. The change of behavior could be due to the increasing competition among higher education institutions to create a student body with high academic ability and high diversity. That is to say, a model built from historical cohorts just does not work for a new cohort, even though it performs perfectly in an out-of-sample test. We developed a transfer learning frameworkIn to address this challenge. Transfer learning requires two data domains, source domain and target domain. The primary interest is to predict the outcome of target domain. A classical learning framework builds a model based on the source domain and applies the model to predict the outcome of the target domain. However, in a transfer learning framework, the model trained from the source domain is modified based on limited information from the target domain before being applied for prediction. That is to say, the knowledge from the source domain is transferred to the target domain instead of simply being applied to the target domain. In this study, we use one cohort of admitted students as source domain and use another cohort as target domain. We have full information of source cohort, including all admitted students' time-dependent and time-independent attributes and week by week admission outcomes between the end of Feburary and the beginning of May. We have limited information in the target cohort, e.g., admission data as of the begining of March. In order to develop a reliable model for the target cohort, the model built from the source cohort is updated according to the limited information in the target cohort. We have succefully shown that the performances of models from the transfer learning framework are better than those from the classical learning framework.

Although a single cohort's data is used as source cohort, we use all available cohorts before the target cohort for variable selection to further enhance models' robustness. Some variables are good predictors for all previous cohorts, but the others are good only for one or two cohorts. Either the classical learning or the transfer learning prefers the first kind of predictors. The second kind of predictors may cause overfitting issues, if they happen to show high predictive powers for a source cohort but just generate white noise for a target cohort. We use Bootstap Lasso (BoLasso) [@bach2008bolasso] to select the first kind of predictors and use them to build the source domain models. We further select a subset of the predictors obtained from BoLasso, by filtering out predictors associates with coefficients with large variance. That is to say, the final set of selected inout variables show consistently good predictive powers. It should be noted that the final set may not have the highest predictive power for a specific target cohort, but we choose to sacrifice a little performance for better robustness.

With the selected set of predictors, we built models from source domain and update them with available data in target domain to predict the admission yield by week. The exception is that the further a week is, the less accurate the predicted yield is. However, the models are able to predict the last week's yield as accuate as 97%, with only two early weeks's data in the target domain are available. The models are updated every week once the past week's data are available, so the accuracy tend to increase every week. With this tool, the Admission Office can monitor the gap between the expected and the actual admission outcomes by week and make adjustment accordingly. 

rolling prediction instead of snapshot prediction
do not assume time independence of variables

link-citations: true
csl: "https://raw.githubusercontent.com/citation-style-language/styles/master/harvard-anglia-ruskin-university.csl"
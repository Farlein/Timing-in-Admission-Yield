


---
title: "In and After Covid-19: Understanding the Timing for Admission Yield"
output: word_document
bibliography: Bib_20220818.bib
pandoc_args: ["--filter=pandoc-citeproc"]
---


## Introduction
Motivation


Estimating the tuition revenue of undergraduate students is a critical part of the budgetary planning [@trusheim2011predictive], as higher education institutions become more reliant on tuition to support operation [@barringer2016changing; @mitchell2016funding]. With tuition rates usualy being stable or stably increasing, predicting undergraduate student tuition revenue relies on acurate enrollment projection. Enrollment projection can be divided into two parts, admission of new students and retention of continuing students. In this project, we build a tool to predict weekly yield in the admission season, between end of Feburary and beginning of May. Institutional administrative units such as the Budget Office and the Admission Office can use this tool to monitor the gap between admission targets and potential outcomes and adjust the expected tuition revenue from incoming students accordingly.

Summary of previous studies
Although predicting admission yield is an important task of enrollment management in higher education institutions, there is scant literature available to describe the modeling strategy. Administrative units of the institutions often consider themselves lack of analytic capabilities and have to relie on consulting companies for the prediction task [@goenner2006predictive], but consulting companies typically view their models as proprietary and reluctant to disclose the modeling details [@desjardins2002analytic]. That being said, there were still intermissive efforts to study admission yield prediction in recent two decades [@aulck2019using; @chang2006applying; @desjardins2002analytic; @goenner2006predictive; @jamison2017applying; @sarafraz2015student;@shrestha2016offer]. Various types of predictive models are used in previous studies, including logistic regression [@chang2006applying; @desjardins2002analytic; @goenner2006predictive; @shrestha2016offer], neural network [@aulck2019using; @chang2006applying; sarafraz2015student; @shrestha2016offer] and tree-based models [@chang2006applying; @jamison2017applying]. The models were built based on training dataset and their performance was evaluated in on test dataset (out-of-sample test). The primary objective is to determine or improve the predictive accuracy of the models in the out-of-sample test, instead of investigating the effects of the input variables. 

Difference from previous studies
We adopt a similar framework from previous studies that we build predictive models for admission yield and test the accuracy out of sample, but we improve the framework in several important aspects. 
First, we predict weekly yields between March and May instead of the final yield. The final yield is surely with the most concern, but it is a better tool if the Admission Office are able to review the predicted yield by week. They gain more confidence of the tool when they can validate the prediction accuracy early in the admission season rather than waiting until the final yield is settled. They can understand better the flow of deposit with the weekly yields and make adjustment to admission decisions swiftly. 
Second, our prediction is active but not static. We update the weekly prediction based on newly available deposit data from the target cohort of applicants every week, instead of making prediction once and done as of a chosen date. The closer to the end of admission season, the more accurate prediction the Admission Office have. 
Third, as a result of the previous consideration, our model is a combination of exponential survival model and logistic regression model. The exponential survival part takes time as input and the logistic regression parts takes applicants' attributes as input, so the output is determined by time such as which week in the admission season and applicants including the academic, demographic and financial information. 
Fourth, we improve prediction accuracy with relatively novel tranfer learning approach [@pan2009survey; @weiss2016survey]. Given the purpose of a model is to predict the yield of a target cohort of applicants, a classical approach build the model based on a historical cohort but does not utilize information from the target cohort, although Feburary deposit data of the target cohort have already been available. A transfer learning approach takes the model built from the historical cohort as input and update the model based on available information from the target cohort, so the update model tends to be more accurate due to the extra information. 
Fifth, we perform variable selection with BootStrap Lasso (BoLasso) [@bach2008bolasso], in order to enhance the models' robustness. This step is to assure input variables have consistent predictive power to suppress overfitting. 
Sixth, we do not assume all input variables are time indepedent. Since our prediction involves target cohort's attributes, we need to predict the the attributes in late weeks based on those in early weeks. It is fine to assume some attributes do not change over time such as applcant's demographic information, but institutional financial aid offered to applicants could change, because the Admission Office can reassign the offered financial aid from applicants who reject the admission offers or just increase the financial aid for highly desired applicants at the end of admission season. 
Seventh, we use one cohort to build a model and use another cohort to test the model in a out-of-sample test. Previous studies randomly split data from a single cohort or mixed cohorts into at least two parts, one part to build a model and the other to test the model. We argue that our out-of-sample test is more realistic. A new cohort never respond to the admission decisions in exactly the same way with the previous cohorts, and we never know how differently a new cohort will behave due to change of admission policies over time or sudden change of macro environment. Either using a single cohort or mixing cohorts together does not reflect enough the modeling challenge from the difference among cohorts and thus potentially does not evaludate the model's performance properly. 

Summary of results
In this paper, we implement the improved framework to nine fall cohorts of applicants to the University of Delaware, from Fall 2011 to Fall 2020. Our models show high accuracy in terms of the difference between the number of predicted deposits and actual deposits by week. The typical error rate is below 3% or below 150 deposits. The closer a future week is to the current week, the more accurate the prediction is. We show that transfer learning helps to enhance the models' predictive accuracy, by comparing the performance of models before and after being updated from target cohort's information. The increase of accuracy depends on the difference between the cohort used to develop a model and the cohort for testing the model. The impact of transfer learning is trivial, if the difference is neglectable. Otherwise, transfer learning is necessary to guarantee this tool is helpful for institutional planning on tuition revenue from incoming students. The accuracy of predicting fall 2020 is an exception though. Unless April's data are available, the accuracy is low for predicting yield in May no matter how the model configuration is tuned.

## Literature Review
### Admission Funnel
An admission funnel invovles six processes, prospects, inquiries, applicants, admits, deposits and enrolls [@stonybrook]. In the prospect stage, the admission office search for high school students who might be interested to the institution. In the inquiry stage, the admission office communicate with students who expressed interest, attempt to furhter enhance their interest, and encourage them to apply. In the applicant stage, the admission office notify students with incomplete application forms, process and review completed applications. In the admit stage, the admission office make decisions to offer admission, put students on a wait list, or reject applications. In the deposit stage, the admission office prepare financial aid packages and collaborate with other offices to interact with admitted students in campus tours and other programs, in order to encourge the students to accept the offers. In the enroll stage, the admission office continue to engage with students and collaborate with other offices to help with new student orientation, course registration and on-campus residency. There are several important rates to monitor in the admission funnel, conversion rate, selection rate, yield rate and melt rate [@ruffalonl]. Conversion rate is The proportion of applicants from inquiries. Selection rate is the proportion of admits from applicants. Yield rate is the proportion of deposits from admits. Melt rate is the proportion of enrolls from deposits. With melt rate typically closed to 1 or 100% at the University of Delaware, the deposit stage or yield rate largely determines the number of new students we will have.   

### Classical Approach
To predict admission outcome, previous studies have built several statistical and data mining models, with logistic regression being the most popular one. DesJardins built a logistic regression model to predict fall 2001 entering cohort based on data of admitted students in fall 1999 in a large public Research I institution in the Midwest [@desjardins2002analytic]. The predicted enrollment probabilities were used to segment admitted students into ten groups, so the admission office could apply different recruitment and marketing strategies. Goenner and Pauls built a logistic regression model to predict the enrollment probabilities of 15,827 inquirers to the University of North Dakota (UND) for fall 2003 [@goenner2006predictive]. They used Bayesian model averaging to address overfiting from a single model. Their results were used at UND to target prospective students for fall 2006 cohort. Thomas et al. built a logistic regression model to predict the enrollment probabilities of admitted students for cohort fall 1998 [@thomas1999using] in Stony Brook University. They used fall 1996 data to train a model, used fall 1997 data to evaluate the model's performance, and then applied the model to fall 1998 data. They suggested to focus on the students with relatively low enrollment probablities (between 30% and 50%), in order to optimize the impact of the recruitment efforts. Shrestha et al. built various models to predict the enrollment acceptance rate of 24,283 admitted international students at a large metropolican Australian University from 2008 to 2013 [@shrestha2016offer]. Logistic regression and neural network models showed better performance than the other models, including Naive Bayes, decision tree, support vector machines, random forests and k-nearest neighbour. Aulck et al. built various models to predict enrollment probabilities and then applied the results to optimize scholarship disbursement [@aulck2019using] in a large, public university. They found that the emsembled classifiers, randome forest, gradient boosted trees and a bagging tree ensemble perform better than the other models, including K-nearest neighbors, regularized logistic regression, support vector machines, and a neural network with 3 hidden layers. They applied the strategy to domestic non-resident applicants in cohort fall 2018, and the admission yield increased from about 10-12% to 14.8%. 

### Transfer Learning
Transfer learning improves the yied prediction for a target cohort by updating a model built from a source cohort, while in a classical approach the model built from the source cohort is directly applied to the target cohort [@pan2009survey; @weiss2016survey]. That is to say, we build a model from the source cohort with known yield information, update the model according to limited information in the target cohort, and apply the updated model for prediction. The update process can be simultaneous with the source model building [@evgeniou2004regularized] or can be in sequence after the source model is built [@bastani2020predicting]. Evgeniou and Pontil (2004) assumed the source task and target task were related by common parameter w₀ in Support Vector Machine (SVM) models, where w₀ and deviation from w₀ were trained simultaneously using labeled source domain and target domain. With experiments on simulated data and real-world data, the authors claimed that the performance of the model was much better than the SVM models trained individually. Bastani (2020) proposed a two-step transfer learning to de-bias parameters in the target model from the source model. There were abundant labels in the source domain and limited labels in the target domain. Linear regression models were used to implement the two-step approach. The baseline approaches include Ridge regression on target data, linear regression on source data, model averaging with source and target data, and weighted loss estimator with source and target data. The author theoretical and empirically proved that the proposed two-step approach has better predictive performance than all baseline approaches. More importantly, the author realized the difference between source model and target model is sparse, when applying the two-step approach to real-world datasets. As a result, the few features could be identified which cause the bias between the source domain and target domain. For example, the price coefficient is the only feature which has non-neglectable difference in models from customer click data (source) and from customer purchase data (target) in e-commerce platforms, and thus business insight was extracted from the proposed 2-step transfer learning approach.    

## Data and Variables
Admit, deposit and yield trend
Data for this study were pulled from a data reporting platform of the University of Delaware (UD), a public research university (Carnegie classification: R1) with a population of about 18,000 undergraduate students. We collected the weekly admission data of out-of-state applicants between late February and early May from Fall 2012 to Fall 2019. We chose late February as the starting point, because most admission decisions were made by the Admission Office by then and the Budget Office began to review the budget plan for the coming fiscal year. Most students paid deposit in early May if they accepted UD's offer, so we used it as the end point. Table 1 shows the number of admits and deposits by May 1 of each year. The number of admitted out-of-state students gradually increased from about 12,500 to about 15,000 from 2012 to 2019, and the number of deposited out-of-state students fluctuated between 2200 and 2900. Figure 1 shows yield by week for Fall 2017, Fall 2018 and Fall 2019. Week 11 always represents May 1, and Week 1 represents 70 days before May 1, which is in late Feburary. For all three falls, the yields increase slowly in early weeks in February and March, increase faster in April, and jump in the last week of April. The yield trends are more similar between Fall 2017 and Fall 2018, and the gap is larger between the trends of Fall 2018 and Fall 2019. Overall the sequence of week is a strong predictor for yield, and thus is used as an independent variable in the models. 

Student attributes
In addition to the sequence of week, student attributes are also included in the initial list of variables to predict yields. Table 2 describes the dependent and indepent variables in the models, and Table 3 shows the descriptive statistics of the variables. The depedent variable is whether a student will pay deposit. The independent variables include the student's demographic information, high school information, financial background, financial aid information. We further include interaction between financial aid and some other variables such as the interaction between institutional aid rate and estimated family contrbution (EFC) rate, because we suspect the effect of institutional aid rate can be affected by other variables. 

## Variable Selection
From the initial list of predictors, we select a subset with consistently high predictive powers and use them as input of the subsequent models. First, we randomly draw samples with replacement from a fall's data with date being May 1. Second, we fit LASSO regression to predict yield and record which variables are selected and the correpsonding coefficients. Third, we repeat the previous two steps 200 times, so we can calculate the probability of each variable being selected for the fall. The higher the probability is, the higher predictive power a variable has. Fourth, we repeat the previous three steps for each fall, so we know which variables tend to have high predictive powers over years. We select Yield from major, HS GPA, Feeder HS, Institutional aid rate, EFC rate, African American, Asian, White, and Inst\*EFC. Fifth, we exclude variables with high variance of coefficients. Even a variable is selected for each fall, the high variance of coefficient indicates the predictive power is not consistent, so it will hurt the predictive performance when a trained model is applied to a test dataset. In this case, we calculate the average coefficients for selected variables from last step for each fall, and the calculate the standard deviation of the average coefficients. All variables show relatively small standard deviation except Yield from major which has standard deviation larger than 1, so we exclude Yield from major. Therefore, the final list of predictors are HS GPA, Feeder HS, Institutional aid rate, EFC rate, African American, Asian, White, and Inst\*EFC.

## Statistical Model
The statistical model is a combination of an expential survival model and a logistic regression model. Equations (1) to (3) describe the prior distribution of unknown coefficients $\alpha_t$, $\beta_t$ and $\beta_j$, where $j$ is from 1 to 8. 

$$ \alpha_t \sim \mathrm{Normal}(\mu_\alpha, \sigma_\alpha) $$ (1)
$$ \beta_t \sim \mathrm{Normal}(\mu_\beta, \sigma_\beta) $$ (2)
$$ \beta_j \sim \mathrm{Normal}(\mu_j, \sigma_j) $$ (3)
$$ \theta_i = exp(\beta_t*(week_i - \alpha_t)) * (1/(1+exp(-\Sigma_j(\beta_j*X_{ij})))) $$ (4)
$$ y_i \sim \mathrm{Bernoulli}(\theta_i) $$ (5)

## Notes
Three key ideas: combination of exponential survival analysis and logistic regression, transfer learning and rolling projection
Need to define source cohort and target cohort in Introduction
Two exponetial functions, maybe cite the material paper?

## Reference

### Last Version ###
It is challenging to build a robust model to predict admission yield, because of potential overfitting, change of admission policies over time or sudden change of macro environment. There are intermissive efforts on admission yield prediction, which usually use out-of-sample tests to evaludate the robustness of the predictive models []. An out-of-sample test requires that the dataset used to test the performance of a model is not used at all when building the model. The dataset used to build the model is called training dataset, and the dataset used to test the model is called test dataset. An overfitted model typically shows high performance from the training dataset, but its performance is much inferior when applied to the test dataset, i.e., in an out-of-sample test. In previous studies, training and test datasets were generated by randomly splitting the available dataset into two datasets. Aulck et al. used a 80-20 training-test split to predict the admission yield of 71,699 applicants between 2014 and 2017 in a public university [@aulck2019using]. They trained 7 predictive classifiers with the training dataset, and tested their performance with the test dataset. All 7 classifiers were robust, with gradient boosted trees, random forests and a neural network with 3 hidden layers performing slightly better. Sarafraz et al. predicted the admission yield of 4082 applicants with a two-layer feed-forward neural network [@sarafraz2015student]. They used a 70-15-15 training-validation-test split and chose the best model using an average of the percentage of correct predictions from all three datasets. Goenner and Pauls used a 50-50 training-test split to predict the yield from 15,827 inquiriers for the fall of 2003 in the University of North Dakota [@goenner2006predictive]. They built multiple logistic regression models from the training dataset, utilized Bayesian model averaging to generate the final model specification, and tested the final model with the test dataset. In this project we also use out-of-sample tests to evalute the models' performance. However, we argue that the regular data split method could undermine the purpose of a out-of-sample test. Admission yield prediction is only meaningful for a new cohort of applicants, and an out-of-sample test is to evaluate a model's potential performance for the new cohort, so the test dataset is supposed to be as independent as possible with the traning dataset. The independence decreases between training and test datasets if they are drawing from the same population, i.e., all available cohorts. Therefore, we use one cohort of applicants' data as training dataset and use another cohort as test dataset, instead of randomly drawing samples from all available cohorts.  

An out-of-sample test helps to detect overfitting and thus helps to increase the robustness when building a predictive model on admssion yield, but it cannot helps to enhance the robustness when a new cohort of admitted students behave distinctly differently from the cohort(s) a model is built from. Similar applicants who accepted an institution's admission offer last year may have non-neglectable lower chance to accpet an offer this year. The change of behavior could be due to the increasing competition among higher education institutions to create a student body with high academic ability and high diversity. That is to say, a model built from historical cohorts just does not work for a new cohort, even though it performs perfectly in an out-of-sample test. We developed a transfer learning frameworkIn to address this challenge. Transfer learning requires two data domains, source domain and target domain. The primary interest is to predict the outcome of target domain. A classical learning framework builds a model based on the source domain and applies the model to predict the outcome of the target domain. However, in a transfer learning framework, the model trained from the source domain is modified based on limited information from the target domain before being applied for prediction. That is to say, the knowledge from the source domain is transferred to the target domain instead of simply being applied to the target domain. In this study, we use one cohort of admitted students as source domain and use another cohort as target domain. We have full information of source cohort, including all admitted students' time-dependent and time-independent attributes and week by week admission outcomes between the end of Feburary and the beginning of May. We have limited information in the target cohort, e.g., admission data as of the begining of March. In order to develop a reliable model for the target cohort, the model built from the source cohort is updated according to the limited information in the target cohort. We have succefully shown that the performances of models from the transfer learning framework are better than those from the classical learning framework.

Although a single cohort's data is used as source cohort, we use all available cohorts before the target cohort for variable selection to further enhance models' robustness. Some variables are good predictors for all previous cohorts, but the others are good only for one or two cohorts. Either the classical learning or the transfer learning prefers the first kind of predictors. The second kind of predictors may cause overfitting issues, if they happen to show high predictive powers for a source cohort but just generate white noise for a target cohort. We use Bootstap Lasso (BoLasso) [@bach2008bolasso] to select the first kind of predictors and use them to build the source domain models. We further select a subset of the predictors obtained from BoLasso, by filtering out predictors associates with coefficients with large variance. That is to say, the final set of selected inout variables show consistently good predictive powers. It should be noted that the final set may not have the highest predictive power for a specific target cohort, but we choose to sacrifice a little performance for better robustness.

With the selected set of predictors, we built models from source domain and update them with available data in target domain to predict the admission yield by week. The exception is that the further a week is, the less accurate the predicted yield is. However, the models are able to predict the last week's yield as accuate as 97%, with only two early weeks's data in the target domain are available. The models are updated every week once the past week's data are available, so the accuracy tend to increase every week. With this tool, the Admission Office can monitor the gap between the expected and the actual admission outcomes by week and make adjustment accordingly. 

rolling prediction instead of snapshot prediction
do not assume time independence of variables

link-citations: true
csl: "https://raw.githubusercontent.com/citation-style-language/styles/master/harvard-anglia-ruskin-university.csl"
